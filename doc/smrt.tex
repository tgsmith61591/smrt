\documentclass[twoside,11pt]{article}

% created using https://www.overleaf.com/8440591sygtwfpygzpt#/29958627/

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

% \jmlrheading{1}{2017}{1-48}{4/00}{10/00}{Marina Meil\u{a} and Michael I. Jordan}

% Short headings should be running head and authors last names

\ShortHeadings{Correcting Class Imbalance with Variational Autoencoders}{Meil\u{a} and Jordan}
\firstpageno{1}

\begin{document}

\title{On the Synthetic Augmentation of Archetypal Minority Class Observations Using Variational Autoencoders}

\author{\name Taylor G.\ Smith \email taylor.smith@alkaline-ml.com \\
       \AND
       \name Jason M.\ White \email jason.m.white5@gmail.com}

\editor{TODO}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
This document describes a methodology by which to remedy imbalanced datasets for the purpose of classification. Datasets may be considered imbalanced if their classification labels are disproportionately represented. In many real-world domains, class imbalance---or the presence of extremely rare events which may be costly-to-misclassify---is quite prevalent. Such commonly cited domains include fraud detection, medical imaging, anomaly detection and countless more.  Our procedure builds on the Synthetic Minority Over-samping Technique (Chawla, Bowyer, Hall \& Kegelmeyer, 2002), but differs in that it generates synthetic examples  only the most prototypical observations in the minority class by leveraging variational autoencoders for anomaly detection.
\end{abstract}

\begin{keywords}
  classification, class-imbalance, cost-sensitive learning, SMOTE, variational autoencoders, skewed distributions
\end{keywords}

\section{Introduction}

Probabilistic inference has become a core technology in AI,
largely due to developments in graph-theoretic methods for the 
representation and manipulation of complex probability 
distributions~\citep{pearl:88}.  Whether in their guise as 
directed graphs (Bayesian networks) or as undirected graphs (Markov 
random fields), \emph{probabilistic graphical models} have a number 
of virtues as representations of uncertainty and as inference engines.  
Graphical models allow a separation between qualitative, structural
aspects of uncertain knowledge and the quantitative, parametric aspects 
of uncertainty...\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}

% Manual newpage inserted to improve layout of sample file - not
% needed in general before appendices/bibliography.

\newpage

\appendix
\section*{Appendix A.}
\label{app:theorem}

% Note: in this sample, the section number is hard-coded in. Following
% proper LaTeX conventions, it should properly be coded as a reference:

%In this appendix we prove the following theorem from
%Section~\ref{sec:textree-generalization}:

In this appendix we prove the following theorem from
Section~6.2:

\noindent
{\bf Theorem} {\it Let $u,v,w$ be discrete variables such that $v, w$ do
not co-occur with $u$ (i.e., $u\neq0\;\Rightarrow \;v=w=0$ in a given
dataset $\dataset$). Let $N_{v0},N_{w0}$ be the number of data points for
which $v=0, w=0$ respectively, and let $I_{uv},I_{uw}$ be the
respective empirical mutual information values based on the sample
$\dataset$. Then
\[
	N_{v0} \;>\; N_{w0}\;\;\Rightarrow\;\;I_{uv} \;\leq\;I_{uw}
\]
with equality only if $u$ is identically 0.} \hfill\BlackBox

\noindent
{\bf Proof}. We use the notation:
\[
P_v(i) \;=\;\frac{N_v^i}{N},\;\;\;i \neq 0;\;\;\;
P_{v0}\;\equiv\;P_v(0)\; = \;1 - \sum_{i\neq 0}P_v(i).
\]
These values represent the (empirical) probabilities of $v$
taking value $i\neq 0$ and 0 respectively.  Entropies will be denoted
by $H$. We aim to show that $\fracpartial{I_{uv}}{P_{v0}} < 0$....\\

{\noindent \em Remainder omitted in this sample. See http://www.jmlr.org/papers/ for full paper.}


\vskip 0.2in
\bibliography{sample}

\end{document}